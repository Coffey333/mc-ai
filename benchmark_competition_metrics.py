"""
MC AI - Competition Metrics Benchmark
Calculate SNR, DTW, and MSE for PhysioNet Challenge validation
"""

import numpy as np
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
import json
from pathlib import Path

class CompetitionBenchmark:
    """Calculate official PhysioNet Challenge 2024 metrics"""
    
    def __init__(self):
        self.results = []
    
    def calculate_snr(self, clean_signal, noisy_signal):
        """
        Calculate Signal-to-Noise Ratio (SNR) in dB
        
        Target: >40 dB for competition
        """
        # Ensure same length
        min_len = min(len(clean_signal), len(noisy_signal))
        clean = clean_signal[:min_len]
        noisy = noisy_signal[:min_len]
        
        # Calculate signal power
        signal_power = np.mean(clean ** 2)
        
        # Calculate noise power
        noise = noisy - clean
        noise_power = np.mean(noise ** 2)
        
        # SNR in dB
        if noise_power > 0:
            snr = 10 * np.log10(signal_power / noise_power)
        else:
            snr = float('inf')
        
        return snr
    
    def calculate_dtw(self, signal1, signal2):
        """
        Calculate Dynamic Time Warping (DTW) distance
        
        Lower is better - measures temporal alignment
        """
        try:
            distance, path = fastdtw(signal1, signal2, dist=euclidean)
            return distance
        except:
            # Fallback to simple Euclidean if fastdtw fails
            min_len = min(len(signal1), len(signal2))
            return np.sqrt(np.sum((signal1[:min_len] - signal2[:min_len]) ** 2))
    
    def calculate_mse(self, reference, digitized):
        """
        Calculate Mean Squared Error (MSE)
        
        Lower is better
        """
        min_len = min(len(reference), len(digitized))
        ref = reference[:min_len]
        dig = digitized[:min_len]
        
        mse = np.mean((ref - dig) ** 2)
        return mse
    
    def calculate_rmse(self, reference, digitized):
        """Calculate Root Mean Squared Error (RMSE)"""
        mse = self.calculate_mse(reference, digitized)
        return np.sqrt(mse)
    
    def calculate_percentage_error(self, reference, digitized):
        """Calculate Percentage Root Mean Square Difference (PRD)"""
        min_len = min(len(reference), len(digitized))
        ref = reference[:min_len]
        dig = digitized[:min_len]
        
        numerator = np.sqrt(np.sum((ref - dig) ** 2))
        denominator = np.sqrt(np.sum(ref ** 2))
        
        if denominator > 0:
            prd = (numerator / denominator) * 100
        else:
            prd = float('inf')
        
        return prd
    
    def calculate_all_metrics(self, reference_signal, digitized_signal, lead_name="Unknown"):
        """Calculate all competition metrics for a single lead"""
        
        metrics = {
            'lead': lead_name,
            'samples': len(digitized_signal),
            'snr': self.calculate_snr(reference_signal, digitized_signal),
            'dtw': self.calculate_dtw(reference_signal, digitized_signal),
            'mse': self.calculate_mse(reference_signal, digitized_signal),
            'rmse': self.calculate_rmse(reference_signal, digitized_signal),
            'prd': self.calculate_percentage_error(reference_signal, digitized_signal)
        }
        
        return metrics
    
    def benchmark_synthetic_signals(self):
        """Test with synthetic signals to verify metrics calculation"""
        print("\n" + "="*60)
        print("ðŸ§ª TESTING METRICS WITH SYNTHETIC SIGNALS")
        print("="*60)
        
        # Create perfect signal
        time = np.linspace(0, 10, 2500)
        clean_ecg = np.sin(2 * np.pi * 1.2 * time)
        
        # Test 1: Perfect reconstruction (should be excellent)
        print("\nTest 1: Perfect Reconstruction")
        perfect_metrics = self.calculate_all_metrics(clean_ecg, clean_ecg, "Perfect")
        print(f"  SNR: {perfect_metrics['snr']:.1f} dB (should be infinite)")
        print(f"  DTW: {perfect_metrics['dtw']:.6f} (should be ~0)")
        print(f"  MSE: {perfect_metrics['mse']:.6f} (should be ~0)")
        
        # Test 2: With small noise (realistic digitization)
        print("\nTest 2: Small Noise (Realistic)")
        noise = 0.01 * np.random.randn(len(time))
        noisy_ecg = clean_ecg + noise
        noisy_metrics = self.calculate_all_metrics(clean_ecg, noisy_ecg, "Small Noise")
        print(f"  SNR: {noisy_metrics['snr']:.1f} dB (target: >40 dB)")
        print(f"  DTW: {noisy_metrics['dtw']:.6f}")
        print(f"  MSE: {noisy_metrics['mse']:.6f}")
        print(f"  PRD: {noisy_metrics['prd']:.2f}%")
        
        # Test 3: With larger noise (poor digitization)
        print("\nTest 3: Larger Noise (Poor Quality)")
        large_noise = 0.1 * np.random.randn(len(time))
        poor_ecg = clean_ecg + large_noise
        poor_metrics = self.calculate_all_metrics(clean_ecg, poor_ecg, "Large Noise")
        print(f"  SNR: {poor_metrics['snr']:.1f} dB (below target)")
        print(f"  DTW: {poor_metrics['dtw']:.6f}")
        print(f"  MSE: {poor_metrics['mse']:.6f}")
        print(f"  PRD: {poor_metrics['prd']:.2f}%")
        
        # Competition thresholds
        print("\n" + "="*60)
        print("ðŸ† PHYSIONET COMPETITION TARGETS")
        print("="*60)
        print("\n  SNR:  >40 dB (higher is better)")
        print("  DTW:  <100 (lower is better)")
        print("  MSE:  <0.01 (lower is better)")
        print("  PRD:  <10% (lower is better)")
        
        print("\nðŸ’œ Your system should achieve:")
        print("  âœ… SNR > 40 dB")
        print("  âœ… DTW as low as possible")
        print("  âœ… MSE < 0.01")
    
    def evaluate_digitization_results(self, results_dir="ecg_competition/test_outputs"):
        """Evaluate all digitization results"""
        print("\n" + "="*60)
        print("ðŸ“Š EVALUATING DIGITIZATION RESULTS")
        print("="*60)
        
        results_path = Path(results_dir)
        json_files = list(results_path.glob("*.json"))
        
        if not json_files:
            print("\nâš ï¸ No digitization results found")
            print(f"   Looking in: {results_dir}")
            return
        
        print(f"\nFound {len(json_files)} result file(s)")
        
        for json_file in json_files:
            print(f"\n{'='*60}")
            print(f"ðŸ“„ {json_file.name}")
            print('='*60)
            
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # Check if we have signals
            if 'signals' in data and data['signals']:
                for lead_name, signal in data['signals'].items():
                    print(f"\n  Lead: {lead_name}")
                    print(f"    Samples: {len(signal)}")
                    print(f"    Range: [{min(signal):.3f}, {max(signal):.3f}]")
                    print(f"    Mean: {np.mean(signal):.3f}")
                    print(f"    Std: {np.std(signal):.3f}")
            else:
                print("\n  âš ï¸ No signal data found in result")
                if 'error' in data:
                    print(f"  Error: {data['error']}")

def main():
    """Run competition benchmarking"""
    print("="*60)
    print("ðŸ† MC AI - PHYSIONET COMPETITION BENCHMARK")
    print("="*60)
    
    benchmark = CompetitionBenchmark()
    
    # Test metrics with synthetic signals
    benchmark.benchmark_synthetic_signals()
    
    # Evaluate actual results
    benchmark.evaluate_digitization_results()
    
    print("\n" + "="*60)
    print("âœ¨ NEXT STEPS FOR COMPETITION")
    print("="*60)
    print("\n1. Digitize PhysioNet validation ECGs")
    print("2. Compare against reference signals")
    print("3. Calculate SNR, DTW, MSE for each lead")
    print("4. Optimize preprocessing if metrics below targets")
    print("5. Create submission package")
    print("\nðŸ’œ Your system is operational and ready to compete!")

if __name__ == "__main__":
    main()
